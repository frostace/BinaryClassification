{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function\n",
    "    * Raise_Question\n",
    "    * Find_best_question\n",
    "    * Partition\n",
    "    * Build_tree\n",
    "    * Predict\n",
    "* Class\n",
    "    * Attribute Node\n",
    "    * Leaf Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation - reorganize the data\n",
    "1. label should be the last column\n",
    "2. make sure that catagorical data are in string format, numerical data are in int or float format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lib\n",
    "# ===========================================================\n",
    "import csv\n",
    "from datascience import *\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize useful data\n",
    "# ===========================================================\n",
    "# with open('clinvar_conflicting_clean.csv', 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     temp_rows = list(reader)\n",
    "df = pd.read_csv('clinvar_conflicting_mapped.csv', low_memory=False)\n",
    "columns_to_change = ['ORIGIN', 'EXON', 'INTRON', 'STRAND', 'LoFtool', 'CADD_PHRED', 'CADD_RAW', 'BLOSUM62']\n",
    "df[['CLNVI', 'MC', 'SYMBOL', 'Feature_type', 'Feature', 'BIOTYPE', \n",
    " 'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', \n",
    " 'BAM_EDIT', 'SIFT', 'PolyPhen']] = df[['CLNVI', 'MC', 'SYMBOL', 'Feature_type', 'Feature', 'BIOTYPE', \n",
    " 'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', \n",
    " 'BAM_EDIT', 'SIFT', 'PolyPhen']].fillna(value=\"null\")\n",
    "df = df.sample(n = df.shape[0])\n",
    "columns_backup = df.columns\n",
    "df.astype({'CLASS': 'int32'})\n",
    "all_rows = df.values.tolist()\n",
    "row_num = len(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find caregorical columns\n",
    "# ===========================================================\n",
    "cate_columns = [0, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 29, 30, 31]\n",
    "# for i in range(len(all_rows[0])):\n",
    "#     if isinstance(all_rows[0][i], str):\n",
    "#         cate_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE Sampling for imbalanced input data\n",
    "# ===========================================================\n",
    "X_train = [row[: -1] for row in all_rows]\n",
    "y_train = [row[-1] for row in all_rows]\n",
    "smt = SMOTENC(random_state=42, categorical_features=cate_columns)\n",
    "X_train, y_train = smt.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the original all_rows with the re-sampled data\n",
    "# ===========================================================\n",
    "all_rows = np.zeros((X_train.shape[0], X_train.shape[1] + 1))\n",
    "all_rows[:, :X_train.shape[1]] = X_train\n",
    "all_rows[:, X_train.shape[1]] = y_train\n",
    "df = pd.DataFrame(all_rows)\n",
    "df.columns = columns_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AF_ESP</th>\n",
       "      <th>AF_EXAC</th>\n",
       "      <th>AF_TGP</th>\n",
       "      <th>CLNDISDB</th>\n",
       "      <th>CLNDN</th>\n",
       "      <th>CLNHGVS</th>\n",
       "      <th>...</th>\n",
       "      <th>Codons</th>\n",
       "      <th>STRAND</th>\n",
       "      <th>BAM_EDIT</th>\n",
       "      <th>SIFT</th>\n",
       "      <th>PolyPhen</th>\n",
       "      <th>LoFtool</th>\n",
       "      <th>CADD_PHRED</th>\n",
       "      <th>CADD_RAW</th>\n",
       "      <th>BLOSUM62</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586422</td>\n",
       "      <td>0.684758</td>\n",
       "      <td>0.567686</td>\n",
       "      <td>0.134764</td>\n",
       "      <td>0.550472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207169</td>\n",
       "      <td>0.016199</td>\n",
       "      <td>0.773532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717244</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.741124</td>\n",
       "      <td>0.298867</td>\n",
       "      <td>0.587580</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765602</td>\n",
       "      <td>0.815242</td>\n",
       "      <td>0.567686</td>\n",
       "      <td>0.905348</td>\n",
       "      <td>0.799610</td>\n",
       "      <td>0.430283</td>\n",
       "      <td>0.147065</td>\n",
       "      <td>0.416847</td>\n",
       "      <td>0.449024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048176</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.121487</td>\n",
       "      <td>0.649386</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491008</td>\n",
       "      <td>0.578522</td>\n",
       "      <td>0.759825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506931</td>\n",
       "      <td>0.476458</td>\n",
       "      <td>0.093744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671769</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.462093</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.329733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.905807</td>\n",
       "      <td>0.578522</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395980</td>\n",
       "      <td>0.787254</td>\n",
       "      <td>0.156162</td>\n",
       "      <td>0.268143</td>\n",
       "      <td>0.149982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325529</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.571376</td>\n",
       "      <td>0.857815</td>\n",
       "      <td>0.814084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.335309</td>\n",
       "      <td>0.578522</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.588318</td>\n",
       "      <td>0.788361</td>\n",
       "      <td>0.515573</td>\n",
       "      <td>0.866147</td>\n",
       "      <td>0.272462</td>\n",
       "      <td>0.257287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981090</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>0.561444</td>\n",
       "      <td>0.993374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CHROM       POS       REF       ALT    AF_ESP   AF_EXAC    AF_TGP  \\\n",
       "0  0.000000  0.586422  0.684758  0.567686  0.134764  0.550472  0.000000   \n",
       "1  0.000000  0.765602  0.815242  0.567686  0.905348  0.799610  0.430283   \n",
       "2  0.000000  0.491008  0.578522  0.759825  0.000000  0.000000  0.000000   \n",
       "3  0.500000  0.905807  0.578522  0.746725  0.000000  0.395980  0.787254   \n",
       "4  0.958333  0.335309  0.578522  0.746725  0.588318  0.788361  0.515573   \n",
       "\n",
       "   CLNDISDB     CLNDN   CLNHGVS  ...    Codons  STRAND  BAM_EDIT  SIFT  \\\n",
       "0  0.207169  0.016199  0.773532  ...  0.717244  0.9375  0.666667   0.8   \n",
       "1  0.147065  0.416847  0.449024  ...  0.048176  0.0625  0.333333   0.8   \n",
       "2  0.506931  0.476458  0.093744  ...  0.671769  0.0625  0.333333   0.8   \n",
       "3  0.156162  0.268143  0.149982  ...  0.325529  0.9375  0.666667   0.8   \n",
       "4  0.866147  0.272462  0.257287  ...  0.981090  0.9375  0.666667   0.8   \n",
       "\n",
       "   PolyPhen   LoFtool  CADD_PHRED  CADD_RAW  BLOSUM62  CLASS  \n",
       "0       0.8  0.741124    0.298867  0.587580  0.000429    0.0  \n",
       "1       0.8  0.121487    0.649386  0.133600  0.000000    1.0  \n",
       "2       0.8  0.462093    0.002304  0.329733  0.000000    0.0  \n",
       "3       0.8  0.571376    0.857815  0.814084  0.000000    1.0  \n",
       "4       0.8  0.008691    0.561444  0.993374  0.000000    1.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Supervised Learning: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision stump part for Adaboost\n",
    "# ===========================================================\n",
    "def is_numeric(value):\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "# === LeafNode is the prediction result of this branch ===\n",
    "class LeafNode:\n",
    "    def __init__(self, rows):\n",
    "        labels = [row[-1] for row in rows]\n",
    "        self.prediction = collections.Counter(labels)\n",
    "\n",
    "# === DecisionNode is an attribute / question used to partition the data ===\n",
    "class DecisionNode:\n",
    "    def __init__(self, question = None, left_branch = None, right_branch = None):\n",
    "        self.question = question\n",
    "        self.left_branch = left_branch\n",
    "        self.right_branch = right_branch\n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, training_attribute, training_data, method = \"CART\"):\n",
    "        self.attribute = training_attribute     # takein attribute and data separately\n",
    "        self.train = training_data[1:]\n",
    "        self.row_num = len(self.train)\n",
    "        self.column_num = len(self.attribute)\n",
    "        self.method = method.upper()            # convert to upper case for general use\n",
    "        self.labels = self.uniq_val(-1)\n",
    "        if self.method not in [\"C4.5\", \"CART\", \"HYBRID\"]:\n",
    "            print(\"Error: Please choose a valid method!\")\n",
    "            return None\n",
    "        self.root = self.build_tree(self.train)\n",
    "    \n",
    "    def uniq_val(self, column):\n",
    "        return set([self.train[i][column] for i in range(len(self.train))])\n",
    "    \n",
    "    # when raising a question.\n",
    "    # if it's a categorical attribute, we simply iterate all categories\n",
    "    # if it's a numeric attribute, we iterate the set of possible numeric values \n",
    "    class Question:\n",
    "        def __init__(self, column, ref_value, attribute):\n",
    "            self.column = column\n",
    "            self.ref_value = ref_value if ref_value else \"None\"\n",
    "            self.attri = attribute\n",
    "            # i hard-coded the categorical columns idx\n",
    "            self.cate_columns = [0, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 29, 30, 31]\n",
    "\n",
    "        def match(self, row):\n",
    "            if is_numeric(self.ref_value) and self.column not in self.cate_columns:\n",
    "                try:\n",
    "                    return row[self.column] >= self.ref_value\n",
    "                except:\n",
    "                    print(\"Error occured in \", row)\n",
    "                    return True\n",
    "            else:\n",
    "                return row[self.column] == self.ref_value\n",
    "\n",
    "        def __repr__(self):\n",
    "            operand = \">=\" if is_numeric(self.ref_value) else \"==\"\n",
    "            return \"Is %s %s %s?\" % (self.attri[self.column], operand, str(self.ref_value))\n",
    "    \n",
    "    # === Method 1 - C4.5 ===\n",
    "    def entropy(self, rows):\n",
    "        # === Bits used to store the information ===\n",
    "        labels = [row[-1] for row in rows]\n",
    "        frequency = collections.Counter(labels).values()\n",
    "        pop = sum(frequency)\n",
    "        H = 0\n",
    "        for f in frequency:\n",
    "            p = f / pop\n",
    "            H -= p * math.log(p, 2)\n",
    "        return H\n",
    "    \n",
    "    # === Method 2 - CART ===\n",
    "    def gini(self, rows):\n",
    "        # === Probability of misclassifying any of your label, which is impurity ===\n",
    "        labels = [row[-1] for row in rows]\n",
    "        frequency = collections.Counter(labels).values()\n",
    "        pop = sum(frequency)\n",
    "        gini = 1\n",
    "        for f in frequency:\n",
    "            p = f / pop\n",
    "            gini -= p ** 2\n",
    "        return gini\n",
    "    \n",
    "    # === Calculate Gain Info ===\n",
    "    def info(self, branches, root):\n",
    "        # === Objective: to find the best question which can maximize info ===\n",
    "        root_size = float(len(root))\n",
    "        if self.method == \"C4.5\":  # Here I pick the GainRatio Approach\n",
    "            root_uncertainty = self.entropy(root)\n",
    "            gain_info = root_uncertainty\n",
    "            split_info = 0\n",
    "            for branch in branches:\n",
    "                if not branch: continue\n",
    "                gain_info -= len(branch) / root_size * self.entropy(branch)\n",
    "                split_info -= float(len(branch)) / root_size * math.log(float(len(branch)) / root_size)\n",
    "#                 print(gain_info, split_info)\n",
    "            return gain_info / split_info\n",
    "        elif self.method == \"CART\":\n",
    "            root_uncertainty = self.gini(root)\n",
    "            gain_info = root_uncertainty\n",
    "            for branch in branches:\n",
    "                if not branch: continue\n",
    "                gain_info -= len(branch) / root_size * self.gini(branch)\n",
    "            return gain_info\n",
    "        elif self.method == \"HYBRID\":\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    # === Here I only do Binary Partitions ===\n",
    "    def partition(self, rows, question):\n",
    "        true_rows = []\n",
    "        false_rows = []\n",
    "        for row in rows:\n",
    "            if question.match(row):\n",
    "                true_rows.append(row)\n",
    "            else:\n",
    "                false_rows.append(row)\n",
    "        return true_rows, false_rows\n",
    "    \n",
    "    def find_best_question(self, rows):\n",
    "        max_info_attenuation = 0\n",
    "        best_question = self.Question(0, self.train[0][0], self.attribute)\n",
    "        # === Iterate through all question candidates ===\n",
    "        # === TODO: Maybe Iteration here can be optimized ===\n",
    "        for col in range(self.column_num - 1): # minus 1 to avoid using the label as attribute\n",
    "            ref_candidates = self.uniq_val(col)\n",
    "            for ref_value in ref_candidates:\n",
    "                if ref_value == \"null\": continue # avoid using null values to generate a question\n",
    "                q = self.Question(col, ref_value, self.attribute)\n",
    "                temp_true_rows, temp_false_rows = self.partition(rows, q)\n",
    "                temp_info_attenuation = self.info([temp_true_rows, temp_false_rows], rows)\n",
    "                if temp_info_attenuation >= max_info_attenuation:\n",
    "                    max_info_attenuation = temp_info_attenuation\n",
    "                    best_question = q\n",
    "        return max_info_attenuation, best_question\n",
    "        \n",
    "    # === Input rows of data with attributes and labels ===\n",
    "    def build_tree(self, rows):\n",
    "        # === Assign all rows as root of the whole decision tree ===\n",
    "        # === We have met the leaf node if gini(rows) is 0 or no question candidates left ===\n",
    "        gain_reduction, q = self.find_best_question(rows)\n",
    "        if gain_reduction <= 0.003:\n",
    "            return LeafNode(rows)\n",
    "        true_rows, false_rows = self.partition(rows, q)\n",
    "        # === Recursion after we have found a optimal question ===\n",
    "        return DecisionNode(q, self.build_tree(true_rows), self.build_tree(false_rows))\n",
    "    \n",
    "    # === Input a row of data with attributes (and no label), predict its label with our decision tree ===\n",
    "    # === Actually it can contain a label, we just don't use it ===\n",
    "    # === walk down the decision tree until we reach the leaf node ===\n",
    "    def classify(self, row, node):\n",
    "        if isinstance(node, LeafNode):\n",
    "#             print(\"===\", node.prediction)\n",
    "            return node.prediction\n",
    "        \n",
    "        if node.question.match(row):\n",
    "#             print(node.question, True)\n",
    "            return self.classify(row, node.left_branch)\n",
    "        else:\n",
    "#             print(node.question, False)\n",
    "            return self.classify(row, node.right_branch)\n",
    "    \n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            print (spacing + \"Predict\", node.prediction)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print (spacing + str(node.question))\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        self.print_tree(node.left_branch, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        self.print_tree(node.right_branch, spacing + \"  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide whole dataset into training set and testing set\n",
    "# ===========================================================\n",
    "training_percentage = 0.01  # percent of partition of training dataset\n",
    "training_size = int(row_num * training_percentage)\n",
    "testing_size = row_num - training_size\n",
    "training_attribute = list(df.columns)\n",
    "training_data = all_rows[: training_size]  # training data should include header row\n",
    "testing_data = all_rows[training_size: ]   # testing data don't need to include header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Trained! Time: 195.872s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ===========================================================\n",
    "start = time.time()\n",
    "tree = DecisionTree(training_attribute, training_data, \"CART\")\n",
    "end = time.time()\n",
    "print(\"Decision Tree Trained! Time: %.03fs\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frostace/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# Testing and Computing TN, TP, FN, FP, etc. \n",
    "# ===========================================================\n",
    "ROC = Table(make_array('CUTOFF', 'TN', 'FN', 'FP', 'TP', 'ACC'))\n",
    "step_size = 0.05\n",
    "CMap = {0: 'TN', 1: 'FN', 2: 'FP', 3: 'TP'}\n",
    "# 00(0) -> TN\n",
    "# 01(1) -> FN\n",
    "# 10(2) -> FP\n",
    "# 11(3) -> TP\n",
    "for cutoff in np.arange(0, 1 + step_size, step_size):\n",
    "    Confusion = {'TN': 0, 'FN': 0, 'FP': 0, 'TP': 0}\n",
    "    for row in testing_data:\n",
    "        # prediction is a counter of label 1 and 0\n",
    "        pred_counter = tree.classify(row, tree.root)\n",
    "        true_rate = pred_counter.get(1, 0) / (pred_counter.get(1, 0) + pred_counter.get(0, 0) + 0.00000001)\n",
    "#         print(true_rate)\n",
    "        true_pred = 1 if true_rate >= cutoff else 0\n",
    "        indicator = (true_pred << 1) + row[-1]\n",
    "        # accordingly update confusion matrix\n",
    "        Confusion[CMap[indicator]] += 1\n",
    "    # concatenate the confusion matrix values into the overall ROC Table\n",
    "    thisline = [cutoff] + list(Confusion.values()) + [(Confusion['TP'] + Confusion['TN']) / sum(Confusion.values())]\n",
    "    ROC = ROC.with_row(thisline)\n",
    "ROC = ROC.with_column('SENSITIVITY', ROC.apply(lambda TP, FN: TP / (TP + FN + 0.00000001), 'TP', 'FN'))\n",
    "ROC = ROC.with_column('FPR', ROC.apply(lambda TN, FP: FP / (TN + FP + 0.00000001), 'TN', 'FP'))\n",
    "ROC = ROC.with_column('FMEAS', ROC.apply(lambda TP, FP, FN: 2 * (TP / (TP + FN)) * (TP / (TP + FP)) / (TP / (TP + FN) + TP / (TP + FP)), 'TP', 'FP', 'FN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>CUTOFF</th> <th>TN</th> <th>FN</th> <th>FP</th> <th>TP</th> <th>ACC</th> <th>SENSITIVITY</th> <th>FPR</th> <th>FMEAS</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0     </td> <td>0    </td> <td>0    </td> <td>48272</td> <td>48585</td> <td>0.501616</td> <td>1          </td> <td>1       </td> <td>0.668101</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.05  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.1   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.15  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.2   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.25  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.3   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.35  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.4   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.45  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.5   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.55  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.6   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.65  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.7   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.75  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.8   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.85  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.9   </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.95  </td> <td>39938</td> <td>39155</td> <td>8334 </td> <td>9430 </td> <td>0.5097  </td> <td>0.194093   </td> <td>0.172647</td> <td>0.284254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1     </td> <td>48272</td> <td>48585</td> <td>0    </td> <td>0    </td> <td>0.498384</td> <td>0          </td> <td>0       </td> <td>nan     </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show\n",
    "ROC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc Curve by cutoff\n",
    "# ===========================================================\n",
    "matplotlib.use('TkAgg')\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Cutoff')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy - Cutoff of Decision Tree')\n",
    "plt.plot(ROC.column('CUTOFF'), ROC.column('ACC'), color='orange')\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree ACC - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_CURVE\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROC - Curve of Decision Tree')\n",
    "plt.plot(ROC.column('FPR'), ROC.column('SENSITIVITY'), color='orange')\n",
    "plt.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), color='black')\n",
    "plt.legend(['Decision Tree', 'Null'])\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree ROC - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.511\n"
     ]
    }
   ],
   "source": [
    "# Compute AUC\n",
    "# ===========================================================\n",
    "length = len(ROC.column('FPR'))\n",
    "auc = 0\n",
    "for i in range(length - 1):\n",
    "    auc += 0.5 * abs(ROC.column('FPR')[i + 1] - ROC.column('FPR')[i]) * (ROC.column('SENSITIVITY')[i] + ROC.column('SENSITIVITY')[i + 1])\n",
    "print(\"auc = %.03f\" %auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5096998668139628\n"
     ]
    }
   ],
   "source": [
    "# Original Testing\n",
    "# ===========================================================\n",
    "\n",
    "accuracy = []\n",
    "for row in testing_data:\n",
    "    classification = tree.classify(row, tree.root)\n",
    "    if len(classification) == 1:\n",
    "        accuracy.append(int(classification.get(row[-1], 0) > 0))\n",
    "    else:\n",
    "        tot = sum(classification.values())\n",
    "        accuracy.append(classification.get(row[-1], 0) / tot)\n",
    "print(sum(accuracy) / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Decision Tree Model - before SMOTE\n",
    "# ===========================================================\n",
    "result = np.array([[0.1, 0.6052853830443929, 0.6341292591846538, 6.658, 0.2575948238372803], \n",
    "[0.2, 0.6696793630299119, 0.6462675119355091, 15.636898589134216, 0.26355624198913574], \n",
    "[0.3, 0.688320280645608, 0.6270048866889123, 15.349877572059631, 0.2788889408111572], \n",
    "[0.4, 0.6926441596845737, 0.6461577795177449, 17.142344093322755, 0.3045179843902588], \n",
    "[0.5, 0.7249279250111773, 0.6423258893248721, 20.893156909942626, 0.3267941474914551], \n",
    "[0.6, 0.6908789196403483, 0.6319173588785352, 23.121678400039674, 0.3292832374572754], \n",
    "[0.7, 0.6911146321003911, 0.6490688659793814, 26.19135489463806, 0.3296070098876953], \n",
    "[0.8, 0.681758435172271, 0.6396528437797009, 29.576567125320434, 0.3475379943847656], \n",
    "[0.9, 0.6796756798972815, 0.6486733420990525, 36.53753626346588, 0.37729477882385254], \n",
    "[1, 0.678894111630874, 0.6433185614453725, 40.28847301006317, 0.3777620792388916]])\n",
    "\n",
    "# SMOTE-Sklearn\n",
    "'''\n",
    "Training: rate = 0.001\n",
    "0.5655408526757759 0.7647023916244506\n",
    "Training: rate = 0.002\n",
    "0.5763186830125472 0.810082483291626\n",
    "Training: rate = 0.003\n",
    "0.5810638166556946 0.8108757019042969\n",
    "Training: rate = 0.004\n",
    "0.5885314771721 0.8401288032531739\n",
    "Training: rate = 0.005\n",
    "0.594624875027056 0.7966373920440674\n",
    "Training: rate = 0.006\n",
    "0.5983048399244761 0.810550308227539\n",
    "Training: rate = 0.007\n",
    "0.5991149071530374 0.7834853887557983\n",
    "Training: rate = 0.008\n",
    "0.5999245306426267 0.8016273975372314\n",
    "Training: rate = 0.009\n",
    "0.602349142614689 0.815482497215271\n",
    "Training: rate = 0.010\n",
    "0.606779028933111 0.8005533218383789\n",
    "Training: rate = 0.011\n",
    "0.6092797295615745 0.822369909286499\n",
    "Training: rate = 0.012\n",
    "0.603282193942162 0.7970266103744507\n",
    "Training: rate = 0.013\n",
    "0.6062842239793851 0.8196890115737915\n",
    "Training: rate = 0.014\n",
    "0.612003994050529 0.8205815076828002\n",
    "Training: rate = 0.015\n",
    "0.6121868687920371 0.8276321887969971\n",
    "Training: rate = 0.016\n",
    "0.6111133113769959 0.8073604106903076\n",
    "Training: rate = 0.017\n",
    "0.6134323063922129 0.8315609693527222\n",
    "Training: rate = 0.018\n",
    "0.6167733648031916 0.7991121768951416\n",
    "Training: rate = 0.019\n",
    "0.6123630509325083 0.8169927835464478\n",
    "Training: rate = 0.020\n",
    "0.6138774356935055 0.8319643974304199\n",
    "Training: rate = 0.021\n",
    "0.620447093577482 0.8392762184143067\n",
    "Training: rate = 0.022\n",
    "0.617440726487212 0.8286631107330322\n",
    "Training: rate = 0.023\n",
    "0.6190865576386119 0.8347836017608643\n",
    "Training: rate = 0.024\n",
    "0.620091837592468 0.8286760091781616\n",
    "Training: rate = 0.025\n",
    "0.6166549210589981 0.8302491903305054\n",
    "Training: rate = 0.026\n",
    "0.6192781106209132 0.8354895114898682\n",
    "Training: rate = 0.027\n",
    "0.6234326910915299 0.8257789850234986\n",
    "Training: rate = 0.028\n",
    "0.6213762687543524 0.8454999923706055\n",
    "Training: rate = 0.029\n",
    "0.6224406163855473 0.8318482160568237\n",
    "Training: rate = 0.030\n",
    "0.6263430003277545 0.8428296327590943\n",
    "Training: rate = 0.031\n",
    "0.6237220328937619 0.8406438112258912\n",
    "Training: rate = 0.032\n",
    "0.6236777980251726 0.8662947177886963\n",
    "Training: rate = 0.033\n",
    "0.626026874250989 0.8328955888748169\n",
    "Training: rate = 0.034\n",
    "0.6257152866985869 0.8468024969100952\n",
    "Training: rate = 0.035\n",
    "0.6265346029586805 0.8474492788314819\n",
    "Training: rate = 0.036\n",
    "0.6265112023660077 0.8620473861694335\n",
    "Training: rate = 0.037\n",
    "0.6261200626191414 0.8301334857940674\n",
    "Training: rate = 0.038\n",
    "0.6280779932411543 0.8524049997329712\n",
    "Training: rate = 0.039\n",
    "0.6299500565598788 0.8645250082015992\n",
    "Training: rate = 0.040\n",
    "0.6299888898384753 0.8536231279373169\n",
    "Training: rate = 0.041\n",
    "0.6284469206831281 0.8789537906646728\n",
    "Training: rate = 0.042\n",
    "0.6301703189063621 0.8516392946243286\n",
    "Training: rate = 0.043\n",
    "0.6301845342706504 0.9003715276718139\n",
    "Training: rate = 0.044\n",
    "0.6306550237078676 0.8652402877807617\n",
    "Training: rate = 0.045\n",
    "0.6300125642980638 0.8881727933883667\n",
    "Training: rate = 0.046\n",
    "0.6320157380432796 0.8888829946517944\n",
    "Training: rate = 0.047\n",
    "0.6337440544088844 0.8905789136886597\n",
    "Training: rate = 0.048\n",
    "0.6314883440341277 0.9063771963119507\n",
    "Training: rate = 0.049\n",
    "0.6352967184652383 0.872174596786499\n",
    "Training: rate = 0.050\n",
    "0.6362192739088661 0.8944581747055054\n",
    "Training: rate = 0.051\n",
    "0.635050142647186 0.8893932819366455\n",
    "Training: rate = 0.052\n",
    "0.636382223760791 0.909731411933899\n",
    "Training: rate = 0.053\n",
    "0.6355432581410209 0.8765779972076416\n",
    "Training: rate = 0.054\n",
    "0.6350010298884469 0.9234981298446655\n",
    "Training: rate = 0.055\n",
    "0.6367308401883967 0.8912766933441162\n",
    "Training: rate = 0.056\n",
    "0.6363679819224752 0.92429518699646\n",
    "Training: rate = 0.057\n",
    "0.6361072745266502 0.9316825151443482\n",
    "Training: rate = 0.058\n",
    "0.6380303310724746 0.9281157970428466\n",
    "Training: rate = 0.059\n",
    "0.6376651118183008 119.9494647026062\n",
    "Training: rate = 0.060\n",
    "0.6377522965807676 0.9300302028656006\n",
    "Training: rate = 0.061\n",
    "0.6386507355751905 53.10061490535736\n",
    "Training: rate = 0.062\n",
    "0.6398423406186109 0.9783363103866577\n",
    "Training: rate = 0.063\n",
    "0.6401926339407871 0.959096097946167\n",
    "Training: rate = 0.064\n",
    "0.6385852653723101 0.9369389057159424\n",
    "Training: rate = 0.065\n",
    "0.6390797411429199 0.9149164199829102\n",
    "Training: rate = 0.066\n",
    "0.641498578063751 1.0091386079788207\n",
    "Training: rate = 0.067\n",
    "0.6404528716680407 1.0058871030807495\n",
    "Training: rate = 0.068\n",
    "0.64043662932723 1.021292805671692\n",
    "Training: rate = 0.069\n",
    "0.6405981493721085 1.0185524940490722\n",
    "Training: rate = 0.070\n",
    "0.6397086554260445 1.0047208070755005\n",
    "Training: rate = 0.071\n",
    "0.6421714411878348 1.0170202732086182\n",
    "Training: rate = 0.072\n",
    "0.6443274246308903 1.0246572017669677\n",
    "Training: rate = 0.073\n",
    "0.6423188405797101 1.0550734043121337\n",
    "Training: rate = 0.074\n",
    "0.6412723023933196 1.0291611909866334\n",
    "Training: rate = 0.075\n",
    "0.6417295858972226 0.9422451972961425\n",
    "Training: rate = 0.076\n",
    "0.6432240449288553 0.969203782081604\n",
    "Training: rate = 0.077\n",
    "0.6443322222222222 0.9420042991638183\n",
    "Training: rate = 0.078\n",
    "0.6418161796603006 0.9857431173324585\n",
    "Training: rate = 0.079\n",
    "0.6444841601247147 0.9456622123718261\n",
    "Training: rate = 0.080\n",
    "0.6430842288313193 0.954564905166626\n",
    "'''\n",
    "\n",
    "[percentage, my_acc, sklearn_acc, my_time_cost, sklearn_time_cost] = result.transpose()\n",
    "# sklearn_acc = [0.5691256634261018, 0.5776658822562248, 0.5827950131665569, 0.5793117650692972, 0.5897413961925768, 0.5923227716847395, 0.5946543283828725, 0.5993445538003475, 0.6014871004129109, 0.6039810220339158]\n",
    "# sklearn_time_cost = [0.7889662265777588, 0.8384890079498291, 0.84961838722229, 0.8846114873886108, 0.8440592765808106, 0.8550214767456055, 0.8227906942367553, 0.8494516849517822, 0.8404701948165894, 0.8558013916015625]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Training Data Percentage(%)')\n",
    "ax1.set_ylabel('Time Consumption(s)', color=color)\n",
    "ax1.plot(percentage, my_time_cost, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('My Model - Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(percentage, my_acc, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('My Decision Tree Performance - SMOTE')\n",
    "# plt.legend(['Time Consumption', 'My ACC'])\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.savefig('Decision Tree Time & Acc by Training percentage - SMOTE.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn Decision Tree Model - before SMOTE\n",
    "# ===========================================================\n",
    "fig, ax3 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax3.set_xlabel('Training Data Percentage(%)')\n",
    "ax3.set_ylabel('Time Consumption(s)', color=color)\n",
    "ax3.plot(percentage, sklearn_time_cost, color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "ax4 = ax3.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax4.set_ylabel('Sklearn - Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax4.plot(percentage, sklearn_acc, color=color)\n",
    "ax4.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Sklearn Decision Tree Performance - SMOTE')\n",
    "# plt.legend(['Time Consumption', 'Sklearn ACC'])\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.savefig('Decision Tree Time & Sklearn Acc by Training percentage - SMOTE.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Models Comparison - Time Consumption\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Training Data Percentage(%)')\n",
    "plt.ylabel('Time Consumption(s)')\n",
    "plt.title('Decision Tree Time Consumption')\n",
    "plt.plot(percentage, my_time_cost, color='orange')\n",
    "plt.plot(percentage, sklearn_time_cost, color='black')\n",
    "plt.legend(['My Model', 'Sklearn Model'])\n",
    "plt.axis([0, 1, -5, 45])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree Time Consumption Comparison - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Models Comparison - Accuracy\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Training Data Percentage(%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Decision Tree Accuracy')\n",
    "plt.plot(percentage, my_acc, color='orange')\n",
    "plt.plot(percentage, sklearn_acc, color='black')\n",
    "plt.legend(['My Model: Avg = 68.0%', 'Sklearn Model: Avg = 64.1%'])\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree Accuracy Comparison - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_distribution {0.0: 48754, 1.0: 48754}\n"
     ]
    }
   ],
   "source": [
    "label_distrib = dict()\n",
    "for row in all_rows:\n",
    "    label_distrib[row[-1]] = label_distrib.get(row[-1], 0) + 1\n",
    "print(\"label_distribution\", label_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === toy data set ===\n",
    "training_data = [\n",
    "    ['Color', 'Diameter', 'Label'],\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "testing_data = [\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Yellow', 3.5, 'Apple'],\n",
    "    ['Green', 3, 'Apple']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.print_tree(tree.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b613ce8aafb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# ===========================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mqq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mall_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# sklearn lib version\n",
    "# ===========================================================\n",
    "from sklearn import tree\n",
    "df = pd.read_csv('clinvar_conflicting_mapped.csv', low_memory=False)\n",
    "df.head()\n",
    "df = df.sample(n = df.shape[0])\n",
    "all_rows = df.values.tolist()\n",
    "row_num = len(all_rows)\n",
    "\n",
    "# SMOTE Sampling for unbalanced input data\n",
    "# ===========================================================\n",
    "X_train = [row[: -1] for row in all_rows]\n",
    "y_train = [row[-1] for row in all_rows]\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)\n",
    "\n",
    "# Concatenate the Resampled Data back to all_rows\n",
    "# ===========================================================\n",
    "for qq in range(len(y_train)):\n",
    "    X_train[qq].append(y_train[qq])\n",
    "all_rows = X_train\n",
    "\n",
    "training_percentage = 0.01  # percent of partition of training dataset\n",
    "training_size = int(row_num * training_percentage)\n",
    "testing_size = row_num - training_size\n",
    "training_attribute = list(df.columns)\n",
    "training_data = all_rows[: training_size]  # training data should include header row\n",
    "testing_data = all_rows[training_size: ]   # testing data don't need to include header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [row[: -1] for row in training_data]\n",
    "Y = [row[-1] for row in training_data]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [row[: -1] for row in testing_data]\n",
    "actual_label = [row[-1] for row in testing_data]\n",
    "result = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i in range(len(result)):\n",
    "    accuracy += int(result[i] == actual_label[i])\n",
    "accuracy /= len(result)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(sklearn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for val in y_train:\n",
    "    counter[val] = counter.get(val, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from numpy.random import RandomState\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "print('Original dataset shape (%s, %s)' % X.shape)\n",
    "\n",
    "print('Original dataset samples per class {}'.format(Counter(y)))\n",
    "\n",
    "# simulate the 2 last columns to be categorical features\n",
    "X[:, -2:] = RandomState(10).randint(0, 4, size=(1000, 2))\n",
    "sm = SMOTENC(random_state=42, categorical_features=[18, 19])\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "print('Resampled dataset samples per class {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(my_time_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
