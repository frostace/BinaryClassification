{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function\n",
    "    * Raise_Question\n",
    "    * Find_best_question\n",
    "    * Partition\n",
    "    * Build_tree\n",
    "    * Predict\n",
    "* Class\n",
    "    * Attribute Node\n",
    "    * Leaf Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation - reorganize the data\n",
    "1. label should be the last column\n",
    "2. make sure that catagorical data are in string format, numerical data are in int or float format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lib\n",
    "# ===========================================================\n",
    "import csv\n",
    "from datascience import *\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize useful data\n",
    "# ===========================================================\n",
    "# with open('clinvar_conflicting_clean.csv', 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     temp_rows = list(reader)\n",
    "df = pd.read_csv('clinvar_conflicting_mapped.csv', low_memory=False)\n",
    "# columns_to_change = ['ORIGIN', 'EXON', 'INTRON', 'STRAND', 'LoFtool', 'CADD_PHRED', 'CADD_RAW', 'BLOSUM62']\n",
    "# df[['CLNVI', 'MC', 'SYMBOL', 'Feature_type', 'Feature', 'BIOTYPE', \n",
    "#  'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', \n",
    "#  'BAM_EDIT', 'SIFT', 'PolyPhen']] = df[['CLNVI', 'MC', 'SYMBOL', 'Feature_type', 'Feature', 'BIOTYPE', \n",
    "#  'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', \n",
    "#  'BAM_EDIT', 'SIFT', 'PolyPhen']].fillna(value=\"null\")\n",
    "df = df.sample(n = df.shape[0])\n",
    "columns_backup = df.columns\n",
    "df.astype({'CLASS': 'int32'})\n",
    "all_rows = df.values.tolist()\n",
    "row_num = len(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find caregorical columns\n",
    "# ===========================================================\n",
    "cate_columns = [0, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 29, 30, 31]\n",
    "# for i in range(len(all_rows[0])):\n",
    "#     if isinstance(all_rows[0][i], str):\n",
    "#         cate_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE Sampling for imbalanced input data\n",
    "# ===========================================================\n",
    "X_train = [row[: -1] for row in all_rows]\n",
    "y_train = [row[-1] for row in all_rows]\n",
    "smt = SMOTENC(random_state=42, categorical_features=cate_columns)\n",
    "X_train, y_train = smt.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the original all_rows with the re-sampled data\n",
    "# ===========================================================\n",
    "all_rows = np.zeros((X_train.shape[0], X_train.shape[1] + 1))\n",
    "all_rows[:, :X_train.shape[1]] = X_train\n",
    "all_rows[:, X_train.shape[1]] = y_train\n",
    "df = pd.DataFrame(all_rows)\n",
    "df.columns = columns_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AF_ESP</th>\n",
       "      <th>AF_EXAC</th>\n",
       "      <th>AF_TGP</th>\n",
       "      <th>CLNDISDB</th>\n",
       "      <th>CLNDN</th>\n",
       "      <th>CLNHGVS</th>\n",
       "      <th>...</th>\n",
       "      <th>Codons</th>\n",
       "      <th>STRAND</th>\n",
       "      <th>BAM_EDIT</th>\n",
       "      <th>SIFT</th>\n",
       "      <th>PolyPhen</th>\n",
       "      <th>LoFtool</th>\n",
       "      <th>CADD_PHRED</th>\n",
       "      <th>CADD_RAW</th>\n",
       "      <th>BLOSUM62</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.181969</td>\n",
       "      <td>0.591224</td>\n",
       "      <td>0.567686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552848</td>\n",
       "      <td>0.898380</td>\n",
       "      <td>0.305961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670419</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.157729</td>\n",
       "      <td>0.585925</td>\n",
       "      <td>0.114138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.943104</td>\n",
       "      <td>0.684758</td>\n",
       "      <td>0.799127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821529</td>\n",
       "      <td>0.840281</td>\n",
       "      <td>0.167255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849617</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.378698</td>\n",
       "      <td>0.305972</td>\n",
       "      <td>0.559103</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.659701</td>\n",
       "      <td>0.578522</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.171710</td>\n",
       "      <td>0.084296</td>\n",
       "      <td>0.903690</td>\n",
       "      <td>0.139701</td>\n",
       "      <td>0.039849</td>\n",
       "      <td>0.638078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344439</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.654216</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.346329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.842700</td>\n",
       "      <td>0.764434</td>\n",
       "      <td>0.567686</td>\n",
       "      <td>0.325123</td>\n",
       "      <td>0.989801</td>\n",
       "      <td>0.126018</td>\n",
       "      <td>0.665259</td>\n",
       "      <td>0.069870</td>\n",
       "      <td>0.833098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764070</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.973558</td>\n",
       "      <td>0.859351</td>\n",
       "      <td>0.412343</td>\n",
       "      <td>0.999747</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.609950</td>\n",
       "      <td>0.578522</td>\n",
       "      <td>0.799127</td>\n",
       "      <td>0.026742</td>\n",
       "      <td>0.262337</td>\n",
       "      <td>0.459990</td>\n",
       "      <td>0.288932</td>\n",
       "      <td>0.232505</td>\n",
       "      <td>0.449837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431337</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>0.899770</td>\n",
       "      <td>0.263903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CHROM       POS       REF       ALT    AF_ESP   AF_EXAC    AF_TGP  \\\n",
       "0  0.125000  0.181969  0.591224  0.567686  0.000000  0.550472  0.000000   \n",
       "1  0.833333  0.943104  0.684758  0.799127  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.659701  0.578522  0.746725  0.171710  0.084296  0.903690   \n",
       "3  0.541667  0.842700  0.764434  0.567686  0.325123  0.989801  0.126018   \n",
       "4  0.416667  0.609950  0.578522  0.799127  0.026742  0.262337  0.459990   \n",
       "\n",
       "   CLNDISDB     CLNDN   CLNHGVS  ...    Codons  STRAND  BAM_EDIT  SIFT  \\\n",
       "0  0.552848  0.898380  0.305961  ...  0.670419  0.9375  0.333333   0.8   \n",
       "1  0.821529  0.840281  0.167255  ...  0.849617  0.9375  0.333333   0.6   \n",
       "2  0.139701  0.039849  0.638078  ...  0.344439  0.0625  0.333333   0.8   \n",
       "3  0.665259  0.069870  0.833098  ...  0.764070  0.9375  0.666667   0.8   \n",
       "4  0.288932  0.232505  0.449837  ...  0.431337  0.0625  0.666667   0.8   \n",
       "\n",
       "   PolyPhen   LoFtool  CADD_PHRED  CADD_RAW  BLOSUM62  CLASS  \n",
       "0       0.8  0.157729    0.585925  0.114138  0.000000    0.0  \n",
       "1       0.2  0.378698    0.305972  0.559103  0.000429    0.0  \n",
       "2       0.8  0.654216    0.702381  0.346329  0.000000    0.0  \n",
       "3       0.8  0.973558    0.859351  0.412343  0.999747    1.0  \n",
       "4       0.8  0.008691    0.899770  0.263903  0.000000    0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Supervised Learning: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision stump part for Adaboost\n",
    "# ===========================================================\n",
    "def is_numeric(value):\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "# === LeafNode is the prediction result of this branch ===\n",
    "class LeafNode:\n",
    "    def __init__(self, rows):\n",
    "        labels = [row[-1] for row in rows]\n",
    "        self.prediction = collections.Counter(labels)\n",
    "\n",
    "# === DecisionNode is an attribute / question used to partition the data ===\n",
    "class DecisionNode:\n",
    "    def __init__(self, question = None, left_branch = None, right_branch = None):\n",
    "        self.question = question\n",
    "        self.left_branch = left_branch\n",
    "        self.right_branch = right_branch\n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, training_attribute, training_data, method = \"CART\"):\n",
    "        self.attribute = training_attribute     # takein attribute and data separately\n",
    "        self.train = training_data[1:]\n",
    "        self.row_num = len(self.train)\n",
    "        self.column_num = len(self.attribute)\n",
    "        self.method = method.upper()            # convert to upper case for general use\n",
    "        self.labels = self.uniq_val(-1)\n",
    "        if self.method not in [\"C4.5\", \"CART\", \"HYBRID\"]:\n",
    "            print(\"Error: Please choose a valid method!\")\n",
    "            return None\n",
    "        self.root = self.build_tree(self.train)\n",
    "    \n",
    "    def uniq_val(self, column):\n",
    "        return set([self.train[i][column] for i in range(len(self.train))])\n",
    "    \n",
    "    # when raising a question.\n",
    "    # if it's a categorical attribute, we simply iterate all categories\n",
    "    # if it's a numeric attribute, we iterate the set of possible numeric values \n",
    "    class Question:\n",
    "        def __init__(self, column, ref_value, attribute):\n",
    "            self.column = column\n",
    "            self.ref_value = ref_value if ref_value else \"None\"\n",
    "            self.attri = attribute\n",
    "            # i hard-coded the categorical columns idx\n",
    "            self.cate_columns = [0, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 29, 30, 31]\n",
    "\n",
    "        def match(self, row):\n",
    "            if is_numeric(self.ref_value) and self.column not in self.cate_columns:\n",
    "                try:\n",
    "                    return row[self.column] >= self.ref_value\n",
    "                except:\n",
    "                    print(\"Error occured in \", row)\n",
    "                    return True\n",
    "            else:\n",
    "                return row[self.column] == self.ref_value\n",
    "\n",
    "        def __repr__(self):\n",
    "            operand = \">=\" if is_numeric(self.ref_value) else \"==\"\n",
    "            return \"Is %s %s %s?\" % (self.attri[self.column], operand, str(self.ref_value))\n",
    "    \n",
    "    # === Method 1 - C4.5 ===\n",
    "    def entropy(self, rows):\n",
    "        # === Bits used to store the information ===\n",
    "        labels = [row[-1] for row in rows]\n",
    "        frequency = collections.Counter(labels).values()\n",
    "        pop = sum(frequency)\n",
    "        H = 0\n",
    "        for f in frequency:\n",
    "            p = f / pop\n",
    "            H -= p * math.log(p, 2)\n",
    "        return H\n",
    "    \n",
    "    # === Method 2 - CART ===\n",
    "    def gini(self, rows):\n",
    "        # === Probability of misclassifying any of your label, which is impurity ===\n",
    "        labels = [row[-1] for row in rows]\n",
    "        frequency = collections.Counter(labels).values()\n",
    "        pop = sum(frequency)\n",
    "        gini = 1\n",
    "        for f in frequency:\n",
    "            p = f / pop\n",
    "            gini -= p ** 2\n",
    "        return gini\n",
    "    \n",
    "    # === Calculate Gain Info ===\n",
    "    def info(self, branches, root):\n",
    "        # === Objective: to find the best question which can maximize info ===\n",
    "        root_size = float(len(root))\n",
    "        if self.method == \"C4.5\":  # Here I pick the GainRatio Approach\n",
    "            root_uncertainty = self.entropy(root)\n",
    "            gain_info = root_uncertainty\n",
    "            split_info = 0\n",
    "            for branch in branches:\n",
    "                if not branch: continue\n",
    "                gain_info -= len(branch) / root_size * self.entropy(branch)\n",
    "                split_info -= float(len(branch)) / root_size * math.log(float(len(branch)) / root_size)\n",
    "#                 print(gain_info, split_info)\n",
    "            return gain_info / split_info\n",
    "        elif self.method == \"CART\":\n",
    "            root_uncertainty = self.gini(root)\n",
    "            gain_info = root_uncertainty\n",
    "            for branch in branches:\n",
    "                if not branch: continue\n",
    "                gain_info -= len(branch) / root_size * self.gini(branch)\n",
    "            return gain_info\n",
    "        elif self.method == \"HYBRID\":\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    # === Here I only do Binary Partitions ===\n",
    "    def partition(self, rows, question):\n",
    "        true_rows = []\n",
    "        false_rows = []\n",
    "        for row in rows:\n",
    "            if question.match(row):\n",
    "                true_rows.append(row)\n",
    "            else:\n",
    "                false_rows.append(row)\n",
    "        return true_rows, false_rows\n",
    "    \n",
    "    def find_best_question(self, rows):\n",
    "        max_info_attenuation = 0\n",
    "        best_question = self.Question(0, self.train[0][0], self.attribute)\n",
    "        # === Iterate through all question candidates ===\n",
    "        # === TODO: Maybe Iteration here can be optimized ===\n",
    "        for col in range(self.column_num - 1): # minus 1 to avoid using the label as attribute\n",
    "            ref_candidates = self.uniq_val(col)\n",
    "            for ref_value in ref_candidates:\n",
    "                if ref_value == \"null\": continue # avoid using null values to generate a question\n",
    "                q = self.Question(col, ref_value, self.attribute)\n",
    "                temp_true_rows, temp_false_rows = self.partition(rows, q)\n",
    "                temp_info_attenuation = self.info([temp_true_rows, temp_false_rows], rows)\n",
    "                if temp_info_attenuation >= max_info_attenuation:\n",
    "                    max_info_attenuation = temp_info_attenuation\n",
    "                    best_question = q\n",
    "        return max_info_attenuation, best_question\n",
    "        \n",
    "    # === Input rows of data with attributes and labels ===\n",
    "    def build_tree(self, rows):\n",
    "        # === Assign all rows as root of the whole decision tree ===\n",
    "        # === We have met the leaf node if gini(rows) is 0 or no question candidates left ===\n",
    "        gain_reduction, q = self.find_best_question(rows)\n",
    "#         if gain_reduction <= 0.003:\n",
    "        if self.gini(rows) <= 0.255:\n",
    "            return LeafNode(rows)\n",
    "        true_rows, false_rows = self.partition(rows, q)\n",
    "        # === Recursion after we have found a optimal question ===\n",
    "        return DecisionNode(q, self.build_tree(true_rows), self.build_tree(false_rows))\n",
    "    \n",
    "    # === Input a row of data with attributes (and no label), predict its label with our decision tree ===\n",
    "    # === Actually it can contain a label, we just don't use it ===\n",
    "    # === walk down the decision tree until we reach the leaf node ===\n",
    "    def classify(self, row, node):\n",
    "        if isinstance(node, LeafNode):\n",
    "#             print(\"===\", node.prediction)\n",
    "            return node.prediction\n",
    "        \n",
    "        if node.question.match(row):\n",
    "#             print(node.question, True)\n",
    "            return self.classify(row, node.left_branch)\n",
    "        else:\n",
    "#             print(node.question, False)\n",
    "            return self.classify(row, node.right_branch)\n",
    "    \n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            print (spacing + \"Predict\", node.prediction)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print (spacing + str(node.question))\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        self.print_tree(node.left_branch, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        self.print_tree(node.right_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide whole dataset into training set and testing set\n",
    "# ===========================================================\n",
    "training_percentage = 0.01  # percent of partition of training dataset\n",
    "training_size = int(row_num * training_percentage)\n",
    "testing_size = row_num - training_size\n",
    "training_attribute = list(df.columns)\n",
    "training_data = all_rows[: training_size]  # training data should include header row\n",
    "testing_data = all_rows[training_size: ]   # testing data don't need to include header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Trained! Time: 60.490s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ===========================================================\n",
    "start = time.time()\n",
    "tree = DecisionTree(training_attribute, training_data, \"CART\")\n",
    "end = time.time()\n",
    "print(\"Decision Tree Trained! Time: %.03fs\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.print_tree(tree.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: [====================] 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frostace/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# Testing and Computing TN, TP, FN, FP, etc. \n",
    "# ===========================================================\n",
    "ROC = Table(make_array('CUTOFF', 'TN', 'FN', 'FP', 'TP', 'ACC'))\n",
    "step_size = 0.05\n",
    "CMap = {0: 'TN', 1: 'FN', 2: 'FP', 3: 'TP'}\n",
    "# 00(0) -> TN\n",
    "# 01(1) -> FN\n",
    "# 10(2) -> FP\n",
    "# 11(3) -> TP\n",
    "for cutoff in np.arange(0, 1 + step_size, step_size):\n",
    "    sys.stdout.write('\\r')\n",
    "    # the exact output you're looking for:\n",
    "    sys.stdout.write(\"Testing: [%-20s] %d%%\" % ('='*int(cutoff * 100 / 5), int(cutoff * 100)))\n",
    "    sys.stdout.flush()\n",
    "    Confusion = {'TN': 0, 'FN': 0, 'FP': 0, 'TP': 0}\n",
    "    for row in testing_data:\n",
    "        # prediction is a counter of label 1 and 0\n",
    "        pred_counter = tree.classify(row, tree.root)\n",
    "        true_rate = pred_counter.get(1, 0) / (pred_counter.get(1, 0) + pred_counter.get(0, 0) + 0.00000001)\n",
    "#         print(true_rate)\n",
    "        true_pred = 1 if true_rate >= cutoff else 0\n",
    "        indicator = (true_pred << 1) + row[-1]\n",
    "        # accordingly update confusion matrix\n",
    "        Confusion[CMap[indicator]] += 1\n",
    "    # concatenate the confusion matrix values into the overall ROC Table\n",
    "    thisline = [cutoff] + list(Confusion.values()) + [(Confusion['TP'] + Confusion['TN']) / sum(Confusion.values())]\n",
    "    ROC = ROC.with_row(thisline)\n",
    "ROC = ROC.with_column('SENSITIVITY', ROC.apply(lambda TP, FN: TP / (TP + FN + 0.00000001), 'TP', 'FN'))\n",
    "ROC = ROC.with_column('FPR', ROC.apply(lambda TN, FP: FP / (TN + FP + 0.00000001), 'TN', 'FP'))\n",
    "ROC = ROC.with_column('FMEAS', ROC.apply(lambda TP, FP, FN: 2 * (TP / (TP + FN)) * (TP / (TP + FP)) / (TP / (TP + FN) + TP / (TP + FP)), 'TP', 'FP', 'FN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>CUTOFF</th> <th>TN</th> <th>FN</th> <th>FP</th> <th>TP</th> <th>ACC</th> <th>SENSITIVITY</th> <th>FPR</th> <th>FMEAS</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0     </td> <td>0    </td> <td>0    </td> <td>48263</td> <td>48594</td> <td>0.501709</td> <td>1          </td> <td>1        </td> <td>0.668184</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.05  </td> <td>4591 </td> <td>6077 </td> <td>43672</td> <td>42517</td> <td>0.486366</td> <td>0.874943   </td> <td>0.904875 </td> <td>0.630896</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.1   </td> <td>7593 </td> <td>9949 </td> <td>40670</td> <td>38645</td> <td>0.477384</td> <td>0.795263   </td> <td>0.842675 </td> <td>0.604258</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.15  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.2   </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.25  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.3   </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.35  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.4   </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.45  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.5   </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.55  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.6   </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.65  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.7   </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.75  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.8   </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.85  </td> <td>41232</td> <td>39063</td> <td>7031 </td> <td>9531 </td> <td>0.524103</td> <td>0.196135   </td> <td>0.145681 </td> <td>0.292559</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.9   </td> <td>42448</td> <td>40924</td> <td>5815 </td> <td>7670 </td> <td>0.517443</td> <td>0.157838   </td> <td>0.120486 </td> <td>0.247104</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.95  </td> <td>43877</td> <td>42964</td> <td>4386 </td> <td>5630 </td> <td>0.511135</td> <td>0.115858   </td> <td>0.0908771</td> <td>0.192117</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1     </td> <td>48263</td> <td>48594</td> <td>0    </td> <td>0    </td> <td>0.498291</td> <td>0          </td> <td>0        </td> <td>nan     </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show\n",
    "ROC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc Curve by cutoff\n",
    "# ===========================================================\n",
    "matplotlib.use('TkAgg')\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Cutoff')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy - Cutoff of Decision Tree')\n",
    "plt.plot(ROC.column('CUTOFF'), ROC.column('ACC'), color='orange')\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree ACC - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_CURVE\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROC - Curve of Decision Tree')\n",
    "plt.plot(ROC.column('FPR'), ROC.column('SENSITIVITY'), color='orange')\n",
    "plt.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), color='black')\n",
    "plt.legend(['Decision Tree', 'Null'])\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree ROC - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.511\n"
     ]
    }
   ],
   "source": [
    "# Compute AUC\n",
    "# ===========================================================\n",
    "length = len(ROC.column('FPR'))\n",
    "auc = 0\n",
    "for i in range(length - 1):\n",
    "    auc += 0.5 * abs(ROC.column('FPR')[i + 1] - ROC.column('FPR')[i]) * (ROC.column('SENSITIVITY')[i] + ROC.column('SENSITIVITY')[i + 1])\n",
    "print(\"auc = %.03f\" %auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5096998668139628\n"
     ]
    }
   ],
   "source": [
    "# Original Testing\n",
    "# ===========================================================\n",
    "\n",
    "accuracy = []\n",
    "for row in testing_data:\n",
    "    classification = tree.classify(row, tree.root)\n",
    "    if len(classification) == 1:\n",
    "        accuracy.append(int(classification.get(row[-1], 0) > 0))\n",
    "    else:\n",
    "        tot = sum(classification.values())\n",
    "        accuracy.append(classification.get(row[-1], 0) / tot)\n",
    "print(sum(accuracy) / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Decision Tree Model - before SMOTE\n",
    "# ===========================================================\n",
    "result = np.array([[0.1, 0.6052853830443929, 0.6341292591846538, 6.658, 0.2575948238372803], \n",
    "[0.2, 0.6696793630299119, 0.6462675119355091, 15.636898589134216, 0.26355624198913574], \n",
    "[0.3, 0.688320280645608, 0.6270048866889123, 15.349877572059631, 0.2788889408111572], \n",
    "[0.4, 0.6926441596845737, 0.6461577795177449, 17.142344093322755, 0.3045179843902588], \n",
    "[0.5, 0.7249279250111773, 0.6423258893248721, 20.893156909942626, 0.3267941474914551], \n",
    "[0.6, 0.6908789196403483, 0.6319173588785352, 23.121678400039674, 0.3292832374572754], \n",
    "[0.7, 0.6911146321003911, 0.6490688659793814, 26.19135489463806, 0.3296070098876953], \n",
    "[0.8, 0.681758435172271, 0.6396528437797009, 29.576567125320434, 0.3475379943847656], \n",
    "[0.9, 0.6796756798972815, 0.6486733420990525, 36.53753626346588, 0.37729477882385254], \n",
    "[1, 0.678894111630874, 0.6433185614453725, 40.28847301006317, 0.3777620792388916]])\n",
    "\n",
    "# SMOTE-Sklearn\n",
    "'''\n",
    "Training: rate = 0.001\n",
    "0.5655408526757759 0.7647023916244506\n",
    "Training: rate = 0.002\n",
    "0.5763186830125472 0.810082483291626\n",
    "Training: rate = 0.003\n",
    "0.5810638166556946 0.8108757019042969\n",
    "Training: rate = 0.004\n",
    "0.5885314771721 0.8401288032531739\n",
    "Training: rate = 0.005\n",
    "0.594624875027056 0.7966373920440674\n",
    "Training: rate = 0.006\n",
    "0.5983048399244761 0.810550308227539\n",
    "Training: rate = 0.007\n",
    "0.5991149071530374 0.7834853887557983\n",
    "Training: rate = 0.008\n",
    "0.5999245306426267 0.8016273975372314\n",
    "Training: rate = 0.009\n",
    "0.602349142614689 0.815482497215271\n",
    "Training: rate = 0.010\n",
    "0.606779028933111 0.8005533218383789\n",
    "Training: rate = 0.011\n",
    "0.6092797295615745 0.822369909286499\n",
    "Training: rate = 0.012\n",
    "0.603282193942162 0.7970266103744507\n",
    "Training: rate = 0.013\n",
    "0.6062842239793851 0.8196890115737915\n",
    "Training: rate = 0.014\n",
    "0.612003994050529 0.8205815076828002\n",
    "Training: rate = 0.015\n",
    "0.6121868687920371 0.8276321887969971\n",
    "Training: rate = 0.016\n",
    "0.6111133113769959 0.8073604106903076\n",
    "Training: rate = 0.017\n",
    "0.6134323063922129 0.8315609693527222\n",
    "Training: rate = 0.018\n",
    "0.6167733648031916 0.7991121768951416\n",
    "Training: rate = 0.019\n",
    "0.6123630509325083 0.8169927835464478\n",
    "Training: rate = 0.020\n",
    "0.6138774356935055 0.8319643974304199\n",
    "Training: rate = 0.021\n",
    "0.620447093577482 0.8392762184143067\n",
    "Training: rate = 0.022\n",
    "0.617440726487212 0.8286631107330322\n",
    "Training: rate = 0.023\n",
    "0.6190865576386119 0.8347836017608643\n",
    "Training: rate = 0.024\n",
    "0.620091837592468 0.8286760091781616\n",
    "Training: rate = 0.025\n",
    "0.6166549210589981 0.8302491903305054\n",
    "Training: rate = 0.026\n",
    "0.6192781106209132 0.8354895114898682\n",
    "Training: rate = 0.027\n",
    "0.6234326910915299 0.8257789850234986\n",
    "Training: rate = 0.028\n",
    "0.6213762687543524 0.8454999923706055\n",
    "Training: rate = 0.029\n",
    "0.6224406163855473 0.8318482160568237\n",
    "Training: rate = 0.030\n",
    "0.6263430003277545 0.8428296327590943\n",
    "Training: rate = 0.031\n",
    "0.6237220328937619 0.8406438112258912\n",
    "Training: rate = 0.032\n",
    "0.6236777980251726 0.8662947177886963\n",
    "Training: rate = 0.033\n",
    "0.626026874250989 0.8328955888748169\n",
    "Training: rate = 0.034\n",
    "0.6257152866985869 0.8468024969100952\n",
    "Training: rate = 0.035\n",
    "0.6265346029586805 0.8474492788314819\n",
    "Training: rate = 0.036\n",
    "0.6265112023660077 0.8620473861694335\n",
    "Training: rate = 0.037\n",
    "0.6261200626191414 0.8301334857940674\n",
    "Training: rate = 0.038\n",
    "0.6280779932411543 0.8524049997329712\n",
    "Training: rate = 0.039\n",
    "0.6299500565598788 0.8645250082015992\n",
    "Training: rate = 0.040\n",
    "0.6299888898384753 0.8536231279373169\n",
    "Training: rate = 0.041\n",
    "0.6284469206831281 0.8789537906646728\n",
    "Training: rate = 0.042\n",
    "0.6301703189063621 0.8516392946243286\n",
    "Training: rate = 0.043\n",
    "0.6301845342706504 0.9003715276718139\n",
    "Training: rate = 0.044\n",
    "0.6306550237078676 0.8652402877807617\n",
    "Training: rate = 0.045\n",
    "0.6300125642980638 0.8881727933883667\n",
    "Training: rate = 0.046\n",
    "0.6320157380432796 0.8888829946517944\n",
    "Training: rate = 0.047\n",
    "0.6337440544088844 0.8905789136886597\n",
    "Training: rate = 0.048\n",
    "0.6314883440341277 0.9063771963119507\n",
    "Training: rate = 0.049\n",
    "0.6352967184652383 0.872174596786499\n",
    "Training: rate = 0.050\n",
    "0.6362192739088661 0.8944581747055054\n",
    "Training: rate = 0.051\n",
    "0.635050142647186 0.8893932819366455\n",
    "Training: rate = 0.052\n",
    "0.636382223760791 0.909731411933899\n",
    "Training: rate = 0.053\n",
    "0.6355432581410209 0.8765779972076416\n",
    "Training: rate = 0.054\n",
    "0.6350010298884469 0.9234981298446655\n",
    "Training: rate = 0.055\n",
    "0.6367308401883967 0.8912766933441162\n",
    "Training: rate = 0.056\n",
    "0.6363679819224752 0.92429518699646\n",
    "Training: rate = 0.057\n",
    "0.6361072745266502 0.9316825151443482\n",
    "Training: rate = 0.058\n",
    "0.6380303310724746 0.9281157970428466\n",
    "Training: rate = 0.059\n",
    "0.6376651118183008 119.9494647026062\n",
    "Training: rate = 0.060\n",
    "0.6377522965807676 0.9300302028656006\n",
    "Training: rate = 0.061\n",
    "0.6386507355751905 53.10061490535736\n",
    "Training: rate = 0.062\n",
    "0.6398423406186109 0.9783363103866577\n",
    "Training: rate = 0.063\n",
    "0.6401926339407871 0.959096097946167\n",
    "Training: rate = 0.064\n",
    "0.6385852653723101 0.9369389057159424\n",
    "Training: rate = 0.065\n",
    "0.6390797411429199 0.9149164199829102\n",
    "Training: rate = 0.066\n",
    "0.641498578063751 1.0091386079788207\n",
    "Training: rate = 0.067\n",
    "0.6404528716680407 1.0058871030807495\n",
    "Training: rate = 0.068\n",
    "0.64043662932723 1.021292805671692\n",
    "Training: rate = 0.069\n",
    "0.6405981493721085 1.0185524940490722\n",
    "Training: rate = 0.070\n",
    "0.6397086554260445 1.0047208070755005\n",
    "Training: rate = 0.071\n",
    "0.6421714411878348 1.0170202732086182\n",
    "Training: rate = 0.072\n",
    "0.6443274246308903 1.0246572017669677\n",
    "Training: rate = 0.073\n",
    "0.6423188405797101 1.0550734043121337\n",
    "Training: rate = 0.074\n",
    "0.6412723023933196 1.0291611909866334\n",
    "Training: rate = 0.075\n",
    "0.6417295858972226 0.9422451972961425\n",
    "Training: rate = 0.076\n",
    "0.6432240449288553 0.969203782081604\n",
    "Training: rate = 0.077\n",
    "0.6443322222222222 0.9420042991638183\n",
    "Training: rate = 0.078\n",
    "0.6418161796603006 0.9857431173324585\n",
    "Training: rate = 0.079\n",
    "0.6444841601247147 0.9456622123718261\n",
    "Training: rate = 0.080\n",
    "0.6430842288313193 0.954564905166626\n",
    "'''\n",
    "\n",
    "[percentage, my_acc, sklearn_acc, my_time_cost, sklearn_time_cost] = result.transpose()\n",
    "# sklearn_acc = [0.5691256634261018, 0.5776658822562248, 0.5827950131665569, 0.5793117650692972, 0.5897413961925768, 0.5923227716847395, 0.5946543283828725, 0.5993445538003475, 0.6014871004129109, 0.6039810220339158]\n",
    "# sklearn_time_cost = [0.7889662265777588, 0.8384890079498291, 0.84961838722229, 0.8846114873886108, 0.8440592765808106, 0.8550214767456055, 0.8227906942367553, 0.8494516849517822, 0.8404701948165894, 0.8558013916015625]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Training Data Percentage(%)')\n",
    "ax1.set_ylabel('Time Consumption(s)', color=color)\n",
    "ax1.plot(percentage, my_time_cost, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('My Model - Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(percentage, my_acc, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('My Decision Tree Performance - SMOTE')\n",
    "# plt.legend(['Time Consumption', 'My ACC'])\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.savefig('Decision Tree Time & Acc by Training percentage - SMOTE.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'percentage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8f44bfac171d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Data Percentage(%)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time Consumption(s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0max3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn_time_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0max3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'percentage' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sklearn Decision Tree Model - before SMOTE\n",
    "# ===========================================================\n",
    "fig, ax3 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax3.set_xlabel('Training Data Percentage(%)')\n",
    "ax3.set_ylabel('Time Consumption(s)', color=color)\n",
    "ax3.plot(percentage, sklearn_time_cost, color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "ax4 = ax3.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax4.set_ylabel('Sklearn - Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax4.plot(percentage, sklearn_acc, color=color)\n",
    "ax4.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Sklearn Decision Tree Performance - SMOTE')\n",
    "# plt.legend(['Time Consumption', 'Sklearn ACC'])\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.savefig('Decision Tree Time & Sklearn Acc by Training percentage - SMOTE.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Models Comparison - Time Consumption\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Training Data Percentage(%)')\n",
    "plt.ylabel('Time Consumption(s)')\n",
    "plt.title('Decision Tree Time Consumption')\n",
    "plt.plot(percentage, my_time_cost, color='orange')\n",
    "plt.plot(percentage, sklearn_time_cost, color='black')\n",
    "plt.legend(['My Model', 'Sklearn Model'])\n",
    "plt.axis([0, 1, -5, 45])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree Time Consumption Comparison - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Models Comparison - Accuracy\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Training Data Percentage(%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Decision Tree Accuracy')\n",
    "plt.plot(percentage, my_acc, color='orange')\n",
    "plt.plot(percentage, sklearn_acc, color='black')\n",
    "plt.legend(['My Model: Avg = 68.0%', 'Sklearn Model: Avg = 64.1%'])\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree Accuracy Comparison - SMOTE.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_distribution {0.0: 48754, 1.0: 48754}\n"
     ]
    }
   ],
   "source": [
    "label_distrib = dict()\n",
    "for row in all_rows:\n",
    "    label_distrib[row[-1]] = label_distrib.get(row[-1], 0) + 1\n",
    "print(\"label_distribution\", label_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === toy data set ===\n",
    "training_data = [\n",
    "    ['Color', 'Diameter', 'Label'],\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "testing_data = [\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Yellow', 3.5, 'Apple'],\n",
    "    ['Green', 3, 'Apple']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn lib version\n",
    "# ===========================================================\n",
    "from sklearn import tree\n",
    "df = pd.read_csv('clinvar_conflicting_clean.csv', low_memory=False)\n",
    "df.fillna(0,inplace=True)\n",
    "df.head()\n",
    "df = df.sample(n = df.shape[0])\n",
    "all_rows = df.values.tolist()\n",
    "row_num = len(all_rows)\n",
    "\n",
    "# balancing \n",
    "# ===========================================================\n",
    "g = df.groupby('CLASS')\n",
    "df_balanced = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "# Extract smaller sample to avoid memory error later, when training starts\n",
    "df_balanced = df_balanced.sample(1000)\n",
    "\n",
    "# spliting data to training and testing\n",
    "# ===========================================================\n",
    "X = df_balanced.drop('CLASS',axis=1)\n",
    "# One hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = df_balanced['CLASS']\n",
    "y = pd.get_dummies(y, drop_first=True)\n",
    "\n",
    "# Train/test split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
    "\n",
    "# Normalize using StandardScaler\n",
    "scaler=StandardScaler()\n",
    "train_X=scaler.fit_transform(train_X)\n",
    "test_X=scaler.transform(test_X)\n",
    "\n",
    "# testing acc\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(train_X, train_y)\n",
    "pred_y = model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(test_y, pred_y)\n",
    "confusion = confusion_matrix(test_y, pred_y)\n",
    "report = classification_report(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.60      0.61      0.61       116\\n           1       0.66      0.65      0.65       134\\n\\n    accuracy                           0.63       250\\n   macro avg       0.63      0.63      0.63       250\\nweighted avg       0.63      0.63      0.63       250\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [row[: -1] for row in testing_data]\n",
    "actual_label = [row[-1] for row in testing_data]\n",
    "result = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i in range(len(result)):\n",
    "    accuracy += int(result[i] == actual_label[i])\n",
    "accuracy /= len(result)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(sklearn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for val in y_train:\n",
    "    counter[val] = counter.get(val, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from numpy.random import RandomState\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "print('Original dataset shape (%s, %s)' % X.shape)\n",
    "\n",
    "print('Original dataset samples per class {}'.format(Counter(y)))\n",
    "\n",
    "# simulate the 2 last columns to be categorical features\n",
    "X[:, -2:] = RandomState(10).randint(0, 4, size=(1000, 2))\n",
    "sm = SMOTENC(random_state=42, categorical_features=[18, 19])\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "print('Resampled dataset samples per class {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(my_time_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conpare balanced and unbalanced\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
