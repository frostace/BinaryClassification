{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function\n",
    "    * Raise_Question\n",
    "    * Find_best_question\n",
    "    * Partition\n",
    "    * Build_tree\n",
    "    * Predict\n",
    "* Class\n",
    "    * Attribute Node\n",
    "    * Leaf Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation - reorganize the data\n",
    "1. label should be the last column\n",
    "2. make sure that catagorical data are in string format, numerical data are in int or float format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lib\n",
    "# ===========================================================\n",
    "import csv\n",
    "from datascience import *\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "# from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize useful data\n",
    "# ===========================================================\n",
    "# with open('clinvar_conflicting_clean.csv', 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     temp_rows = list(reader)\n",
    "df = pd.read_csv('clinvar_conflicting_clean.csv', low_memory=False)\n",
    "columns_to_change = ['ORIGIN', 'EXON', 'INTRON', 'STRAND', 'LoFtool', 'CADD_PHRED', 'CADD_RAW', 'BLOSUM62']\n",
    "df[['CLNVI', 'MC', 'SYMBOL', 'Feature_type', 'Feature', 'BIOTYPE', \n",
    " 'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', \n",
    " 'BAM_EDIT', 'SIFT', 'PolyPhen']] = df[['CLNVI', 'MC', 'SYMBOL', 'Feature_type', 'Feature', 'BIOTYPE', \n",
    " 'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', \n",
    " 'BAM_EDIT', 'SIFT', 'PolyPhen']].fillna(value=\"null\")\n",
    "df = df.sample(n = df.shape[0])\n",
    "all_rows = df.values.tolist()\n",
    "row_num = len(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Supervised Learning: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision stump part for Adaboost\n",
    "# ===========================================================\n",
    "def is_numeric(value):\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "# === LeafNode is the prediction result of this branch ===\n",
    "class LeafNode:\n",
    "    def __init__(self, rows):\n",
    "        labels = [row[-1] for row in rows]\n",
    "        self.prediction = collections.Counter(labels)\n",
    "\n",
    "# === DecisionNode is an attribute / question used to partition the data ===\n",
    "class DecisionNode:\n",
    "    def __init__(self, question = None, left_branch = None, right_branch = None):\n",
    "        self.question = question\n",
    "        self.left_branch = left_branch\n",
    "        self.right_branch = right_branch\n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, training_attribute, training_data, method = \"CART\"):\n",
    "        self.attribute = training_attribute     # takein attribute and data separately\n",
    "        self.train = training_data[1:]\n",
    "        self.row_num = len(self.train)\n",
    "        self.column_num = len(self.attribute)\n",
    "        self.method = method.upper()            # convert to upper case for general use\n",
    "        self.labels = self.uniq_val(-1)\n",
    "        if self.method not in [\"C4.5\", \"CART\", \"HYBRID\"]:\n",
    "            print(\"Error: Please choose a valid method!\")\n",
    "            return None\n",
    "        self.root = self.build_tree(self.train)\n",
    "    \n",
    "    def uniq_val(self, column):\n",
    "        return set([self.train[i][column] for i in range(len(self.train))])\n",
    "    \n",
    "    # when raising a question.\n",
    "    # if it's a categorical attribute, we simply iterate all categories\n",
    "    # if it's a numeric attribute, we iterate the set of possible numeric values \n",
    "    class Question:\n",
    "        def __init__(self, column, ref_value, attribute):\n",
    "            self.column = column\n",
    "            self.ref_value = ref_value if ref_value else \"None\"\n",
    "            self.attri = attribute\n",
    "\n",
    "        def match(self, row):\n",
    "            if is_numeric(self.ref_value):\n",
    "                try:\n",
    "                    return row[self.column] >= self.ref_value\n",
    "                except:\n",
    "                    print(\"Error occured in \", row)\n",
    "                    return True\n",
    "            else:\n",
    "                return row[self.column] == self.ref_value\n",
    "\n",
    "        def __repr__(self):\n",
    "            operand = \">=\" if is_numeric(self.ref_value) else \"==\"\n",
    "            return \"Is %s %s %s?\" % (self.attri[self.column], operand, str(self.ref_value))\n",
    "    \n",
    "    # === Method 1 - C4.5 ===\n",
    "    def entropy(self, rows):\n",
    "        # === Bits used to store the information ===\n",
    "        labels = [row[-1] for row in rows]\n",
    "        frequency = collections.Counter(labels).values()\n",
    "        pop = sum(frequency)\n",
    "        H = 0\n",
    "        for f in frequency:\n",
    "            p = f / pop\n",
    "            H -= p * math.log(p, 2)\n",
    "        return H\n",
    "    \n",
    "    # === Method 2 - CART ===\n",
    "    def gini(self, rows):\n",
    "        # === Probability of misclassifying any of your label, which is impurity ===\n",
    "        labels = [row[-1] for row in rows]\n",
    "        frequency = collections.Counter(labels).values()\n",
    "        pop = sum(frequency)\n",
    "        gini = 1\n",
    "        for f in frequency:\n",
    "            p = f / pop\n",
    "            gini -= p ** 2\n",
    "        return gini\n",
    "    \n",
    "    # === Calculate Gain Info ===\n",
    "    # I'm actually returning the gain info reduction\n",
    "    def info(self, branches, root):\n",
    "        # === Objective: to find the best question which can maximize info ===\n",
    "        root_size = float(len(root))\n",
    "        if self.method == \"C4.5\":  # Here I pick the GainRatio Approach\n",
    "            root_uncertainty = self.entropy(root)\n",
    "            gain_info = root_uncertainty\n",
    "            split_info = 0\n",
    "            for branch in branches:\n",
    "                if not branch: continue\n",
    "                gain_info -= len(branch) / root_size * self.entropy(branch)\n",
    "                split_info -= float(len(branch)) / root_size * math.log(float(len(branch)) / root_size)\n",
    "#                 print(gain_info, split_info)\n",
    "            return gain_info / split_info\n",
    "        elif self.method == \"CART\":\n",
    "            root_uncertainty = self.gini(root)\n",
    "            gain_info = root_uncertainty\n",
    "            for branch in branches:\n",
    "                if not branch: continue\n",
    "                gain_info -= len(branch) / root_size * self.gini(branch)\n",
    "            return gain_info\n",
    "        elif self.method == \"HYBRID\":\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    # === Here I only do Binary Partitions ===\n",
    "    def partition(self, rows, question):\n",
    "        true_rows = []\n",
    "        false_rows = []\n",
    "        for row in rows:\n",
    "            if question.match(row):\n",
    "                true_rows.append(row)\n",
    "            else:\n",
    "                false_rows.append(row)\n",
    "        return true_rows, false_rows\n",
    "    \n",
    "    def find_best_question(self, rows):\n",
    "        max_info_attenuation = 0\n",
    "        best_question = self.Question(0, self.train[0][0], self.attribute)\n",
    "        # === Iterate through all question candidates ===\n",
    "        # === TODO: Maybe Iteration here can be optimized ===\n",
    "        for col in range(self.column_num - 1): # minus 1 to avoid using the label as attribute\n",
    "            ref_candidates = self.uniq_val(col)\n",
    "            for ref_value in ref_candidates:\n",
    "                if ref_value == \"null\": continue # avoid using null values to generate a question\n",
    "                q = self.Question(col, ref_value, self.attribute)\n",
    "                temp_true_rows, temp_false_rows = self.partition(rows, q)\n",
    "                temp_info_attenuation = self.info([temp_true_rows, temp_false_rows], rows)\n",
    "                if temp_info_attenuation >= max_info_attenuation:\n",
    "                    max_info_attenuation = temp_info_attenuation\n",
    "                    best_question = q\n",
    "        return max_info_attenuation, best_question\n",
    "        \n",
    "    # === Input rows of data with attributes and labels ===\n",
    "    def build_tree(self, rows):\n",
    "        # === Assign all rows as root of the whole decision tree ===\n",
    "        # === We have met the leaf node if gini(rows) is 0 or no question candidates left ===\n",
    "        gain_reduction, q = self.find_best_question(rows)\n",
    "#         if gain_reduction <= 0.003:\n",
    "        if self.gini(rows) <= 0.48:\n",
    "            return LeafNode(rows)\n",
    "        true_rows, false_rows = self.partition(rows, q)\n",
    "        # === Recursion after we have found a optimal question ===\n",
    "        return DecisionNode(q, self.build_tree(true_rows), self.build_tree(false_rows))\n",
    "    \n",
    "    # === Input a row of data with attributes (and no label), predict its label with our decision tree ===\n",
    "    # === Actually it can contain a label, we just don't use it ===\n",
    "    # === walk down the decision tree until we reach the leaf node ===\n",
    "    def classify(self, row, node):\n",
    "        if isinstance(node, LeafNode):\n",
    "#             print(\"===\", node.prediction)\n",
    "            return node.prediction\n",
    "        \n",
    "        if node.question.match(row):\n",
    "#             print(node.question, True)\n",
    "            return self.classify(row, node.left_branch)\n",
    "        else:\n",
    "#             print(node.question, False)\n",
    "            return self.classify(row, node.right_branch)\n",
    "    \n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            print (spacing + \"Predict\", node.prediction)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print (spacing + str(node.question))\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        self.print_tree(node.left_branch, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        self.print_tree(node.right_branch, spacing + \"  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide whole dataset into training set and testing set\n",
    "# ===========================================================\n",
    "training_percentage = 0.01  # percent of partition of training dataset\n",
    "training_size = int(row_num * training_percentage)\n",
    "testing_size = row_num - training_size\n",
    "training_attribute = list(df.columns)      # i omitted the 'CLASS' inside the decision tree structure\n",
    "training_data = all_rows[: training_size]  # training data should include header row\n",
    "testing_data = all_rows[training_size: ]   # testing data don't need to include header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Trained! Time: 3.477s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ===========================================================\n",
    "start = time.time()\n",
    "tree = DecisionTree(training_attribute, training_data, \"CART\")\n",
    "end = time.time()\n",
    "print(\"Decision Tree Trained! Time: %.03fs\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3799424730002998\n"
     ]
    }
   ],
   "source": [
    "print(tree.gini(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.print_tree(tree.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: [====================] 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frostace/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# Testing and Computing TN, TP, FN, FP, etc. \n",
    "# ===========================================================\n",
    "ROC = Table(make_array('CUTOFF', 'TN', 'FN', 'FP', 'TP', 'ACC'))\n",
    "step_size = 0.05\n",
    "CMap = {0: 'TN', 1: 'FN', 2: 'FP', 3: 'TP'}\n",
    "# 00(0) -> TN\n",
    "# 01(1) -> FN\n",
    "# 10(2) -> FP\n",
    "# 11(3) -> TP\n",
    "for cutoff in np.arange(0, 1 + step_size, step_size):\n",
    "    sys.stdout.write('\\r')\n",
    "    # the exact output you're looking for:\n",
    "    sys.stdout.write(\"Testing: [%-20s] %d%%\" % ('='*int(cutoff * 100 / 5), int(cutoff * 100)))\n",
    "    sys.stdout.flush()\n",
    "    Confusion = {'TN': 0, 'FN': 0, 'FP': 0, 'TP': 0}\n",
    "    for row in testing_data:\n",
    "        # prediction is a counter of label 1 and 0\n",
    "        pred_counter = tree.classify(row, tree.root)\n",
    "        true_rate = pred_counter.get(1, 0) / (pred_counter.get(1, 0) + pred_counter.get(0, 0) + 0.00000001)\n",
    "#         print(true_rate)\n",
    "        true_pred = 1 if true_rate >= cutoff else 0\n",
    "        indicator = (true_pred << 1) + row[-1]\n",
    "        # accordingly update confusion matrix\n",
    "        Confusion[CMap[indicator]] += 1\n",
    "    # concatenate the confusion matrix values into the overall ROC Table\n",
    "    thisline = [cutoff] + list(Confusion.values()) + [(Confusion['TP'] + Confusion['TN']) / sum(Confusion.values())]\n",
    "    ROC = ROC.with_row(thisline)\n",
    "ROC = ROC.with_column('SENSITIVITY', ROC.apply(lambda TP, FN: TP / (TP + FN + 0.00000001), 'TP', 'FN'))\n",
    "ROC = ROC.with_column('FPR', ROC.apply(lambda TN, FP: FP / (TN + FP + 0.00000001), 'TN', 'FP'))\n",
    "ROC = ROC.with_column('FMEAS', ROC.apply(lambda TP, FP, FN: 2 * (TP / (TP + FN)) * (TP / (TP + FP)) / (TP / (TP + FN) + TP / (TP + FP)), 'TP', 'FP', 'FN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>CUTOFF</th> <th>TN</th> <th>FN</th> <th>FP</th> <th>TP</th> <th>ACC</th> <th>SENSITIVITY</th> <th>FPR</th> <th>FMEAS</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0     </td> <td>0    </td> <td>0    </td> <td>48265</td> <td>16272</td> <td>0.252134</td> <td>1          </td> <td>1   </td> <td>0.402727</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.05  </td> <td>0    </td> <td>0    </td> <td>48265</td> <td>16272</td> <td>0.252134</td> <td>1          </td> <td>1   </td> <td>0.402727</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.1   </td> <td>0    </td> <td>0    </td> <td>48265</td> <td>16272</td> <td>0.252134</td> <td>1          </td> <td>1   </td> <td>0.402727</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.15  </td> <td>0    </td> <td>0    </td> <td>48265</td> <td>16272</td> <td>0.252134</td> <td>1          </td> <td>1   </td> <td>0.402727</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.2   </td> <td>0    </td> <td>0    </td> <td>48265</td> <td>16272</td> <td>0.252134</td> <td>1          </td> <td>1   </td> <td>0.402727</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.25  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.3   </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.35  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.4   </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.45  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.5   </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.55  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.6   </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.65  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.7   </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.75  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.8   </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.85  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.9   </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.95  </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1     </td> <td>48265</td> <td>16272</td> <td>0    </td> <td>0    </td> <td>0.747866</td> <td>0          </td> <td>0   </td> <td>nan     </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show\n",
    "ROC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc Curve by cutoff\n",
    "# ===========================================================\n",
    "matplotlib.use('TkAgg')\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Cutoff')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy - Cutoff of Decision Tree')\n",
    "plt.plot(ROC.column('CUTOFF'), ROC.column('ACC'), color='orange')\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree ACC.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_CURVE\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROC - Curve of Decision Tree')\n",
    "plt.plot(ROC.column('FPR'), ROC.column('SENSITIVITY'), color='orange')\n",
    "plt.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), color='black')\n",
    "plt.legend(['Decision Tree', 'Null'])\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree ROC.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.555\n"
     ]
    }
   ],
   "source": [
    "# Compute AUC\n",
    "# ===========================================================\n",
    "length = len(ROC.column('FPR'))\n",
    "auc = 0\n",
    "for i in range(length - 1):\n",
    "    auc += 0.5 * abs(ROC.column('FPR')[i + 1] - ROC.column('FPR')[i]) * (ROC.column('SENSITIVITY')[i] + ROC.column('SENSITIVITY')[i + 1])\n",
    "print(\"auc = %.03f\" %auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7072377086012674\n"
     ]
    }
   ],
   "source": [
    "# Original Testing\n",
    "# ===========================================================\n",
    "\n",
    "accuracy = []\n",
    "for row in testing_data:\n",
    "    classification = tree.classify(row, tree.root)\n",
    "    if len(classification) == 1:\n",
    "        accuracy.append(int(classification.get(row[-1], 0) > 0))\n",
    "    else:\n",
    "        tot = sum(classification.values())\n",
    "        accuracy.append(classification.get(row[-1], 0) / tot)\n",
    "print(sum(accuracy) / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Decision Tree Model - before SMOTE\n",
    "# ===========================================================\n",
    "result = np.array([[0.1, 0.6052853830443929, 0.6341292591846538, 6.658, 0.2575948238372803], \n",
    "[0.2, 0.6696793630299119, 0.6462675119355091, 15.636898589134216, 0.26355624198913574], \n",
    "[0.3, 0.688320280645608, 0.6270048866889123, 15.349877572059631, 0.2788889408111572], \n",
    "[0.4, 0.6926441596845737, 0.6461577795177449, 17.142344093322755, 0.3045179843902588], \n",
    "[0.5, 0.7249279250111773, 0.6423258893248721, 20.893156909942626, 0.3267941474914551], \n",
    "[0.6, 0.6908789196403483, 0.6319173588785352, 23.121678400039674, 0.3292832374572754], \n",
    "[0.7, 0.6911146321003911, 0.6490688659793814, 26.19135489463806, 0.3296070098876953], \n",
    "[0.8, 0.681758435172271, 0.6396528437797009, 29.576567125320434, 0.3475379943847656], \n",
    "[0.9, 0.6796756798972815, 0.6486733420990525, 36.53753626346588, 0.37729477882385254], \n",
    "[1, 0.678894111630874, 0.6433185614453725, 40.28847301006317, 0.3777620792388916]])\n",
    "\n",
    "[percentage, my_acc, sklearn_acc, my_time_cost, sklearn_time_cost] = result.transpose()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Training Data Percentage(%)')\n",
    "ax1.set_ylabel('Time Consumption(s)', color=color)\n",
    "ax1.plot(percentage, my_time_cost, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('My Model - Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(percentage, my_acc, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('My Decision Tree Performance')\n",
    "# plt.legend(['Time Consumption', 'My ACC'])\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.savefig('Decision Tree Time & Acc by Training percentage.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn Decision Tree Model - before SMOTE\n",
    "# ===========================================================\n",
    "fig, ax3 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax3.set_xlabel('Training Data Percentage(%)')\n",
    "ax3.set_ylabel('Time Consumption(s)', color=color)\n",
    "ax3.plot(percentage, sklearn_time_cost, color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "ax4 = ax3.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax4.set_ylabel('Sklearn - Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax4.plot(percentage, sklearn_acc, color=color)\n",
    "ax4.tick_params(axis='y', labelcolor=color)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Sklearn Decision Tree Performance')\n",
    "# plt.legend(['Time Consumption', 'Sklearn ACC'])\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.savefig('Decision Tree Time & Sklearn Acc by Training percentage.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Models Comparison - Time Consumption\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Training Data Percentage(%)')\n",
    "plt.ylabel('Time Consumption(s)')\n",
    "plt.title('Decision Tree Time Consumption')\n",
    "plt.plot(percentage, my_time_cost, color='orange')\n",
    "plt.plot(percentage, sklearn_time_cost, color='black')\n",
    "plt.legend(['My Model', 'Sklearn Model'])\n",
    "plt.axis([0, 1, -5, 45])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree Time Consumption Comparison.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Models Comparison - Accuracy\n",
    "# ===========================================================\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Training Data Percentage(%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Decision Tree Accuracy')\n",
    "plt.plot(percentage, my_acc, color='orange')\n",
    "plt.plot(percentage, sklearn_acc, color='black')\n",
    "plt.legend(['My Model: Avg = 68.0%', 'Sklearn Model: Avg = 64.1%'])\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()\n",
    "fig.savefig('Decision Tree Accuracy Comparison.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_distribution {0: 48754, 1: 16434}\n"
     ]
    }
   ],
   "source": [
    "label_distrib = dict()\n",
    "for row in all_rows:\n",
    "    label_distrib[row[-1]] = label_distrib.get(row[-1], 0) + 1\n",
    "print(\"label_distribution\", label_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === toy data set ===\n",
    "training_data = [\n",
    "    ['Color', 'Diameter', 'Label'],\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "testing_data = [\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Yellow', 3.5, 'Apple'],\n",
    "    ['Green', 3, 'Apple']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.print_tree(tree.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [row[: -1] for row in training_data]\n",
    "Y = [row[-1] for row in training_data]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [row[: -1] for row in testing_data]\n",
    "actual_label = [row[-1] for row in testing_data]\n",
    "result = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i in range(len(result)):\n",
    "    accuracy += int(result[i] == actual_label[i])\n",
    "accuracy /= len(result)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(sklearn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100%"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "for i in range(101):\n",
    "    sys.stdout.write('\\r')\n",
    "    # the exact output you're looking for:\n",
    "    sys.stdout.write(\"[%-20s] %d%%\" % ('='*(i // 5), i))\n",
    "    sys.stdout.flush()\n",
    "    sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
